{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try write here the LTSM version lol \n",
    "# hyperparameters\n",
    "hidden_size = 512 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters \n",
    "def init_weight(*shape):\n",
    "    fan_in = shape[1] if len(shape) > 1 else shape[0]\n",
    "    return np.random.randn(*shape) * np.sqrt(2.0 / fan_in)  # He initialization\n",
    "\n",
    "def init_weight_orthogonal(*shape, gain=1.0):\n",
    "    W = np.random.randn(*shape)\n",
    "    u, _, v = np.linalg.svd(W, full_matrices=False)\n",
    "    W = u if u.shape == shape else v.T  # Handle non-square\n",
    "    return gain * W  # Scale for activation functions (e.g., ReLU: gain=√2)\n",
    "\n",
    "\n",
    "triple = lambda: (init_weight(hidden_size, vocab_size), # input to hidden\n",
    "                init_weight_orthogonal(hidden_size, hidden_size), # hidden to hidden\n",
    "                np.zeros((hidden_size, 1))) #hidden bias\n",
    "Wxi, Whi, bi = triple()  # Input gate\n",
    "Wxf, Whf, bf = triple()  # Forget gate\n",
    "Wxo, Who, bo = triple()  # Output gate\n",
    "Wxc, Whc, bc = triple()  # Input node\n",
    "\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "by = np.zeros((vocab_size, 1)) #output bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM3-chain.png\" alt=\"LSTM Chain\"/>\n",
    "<img src=\"images/ltsm.png\" alt=\"LSTM \"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "def lossFunLTSM(inputs, targets, hprev, cprev):\n",
    "    \"\"\"\n",
    "        inputs, targets are both list of integers.\n",
    "        hprev is Hx1 array of initial hidden state  \n",
    "        cprev is Hx1 array of initial cell state\n",
    "        returns the loss, gradients on model parameters, and last hidden and cell state\n",
    "    \"\"\"\n",
    "    xs, hs, cs, ys, ps, i_s, f_s, o_s, c_tildes = {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    cs[-1] = np.copy(cprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "\n",
    "        #LTSM forward pass\n",
    "        i_s[t] = sigmoid(np.dot(Wxi, xs[t]) + np.dot(Whi, hs[t-1]) + bi) # (hidden,1)\n",
    "        f_s[t] = sigmoid(np.dot(Wxf, xs[t]) + np.dot(Whf, hs[t-1]) + bf) # (hidden, 1)\n",
    "        o_s[t] = sigmoid(np.dot(Wxo, xs[t]) + np.dot(Who, hs[t-1]) + bo) # (hidden, 1)\n",
    "        c_tildes[t] = np.tanh(np.dot(Wxc, xs[t]) + np.dot(Whc, hs[t-1]) + bc) # (hidden, 1)\n",
    "\n",
    "        cs[t] = f_s[t] * cs[t-1] + i_s[t] * c_tildes[t] # memory cell, internal state elementwise multiply (hidden, 1)\n",
    "        hs[t] = o_s[t] * np.tanh(cs[t]) # hidden state, elementwise multiply (hidden, 1)\n",
    "\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars (vocab, 1)\n",
    "        ps[t] = softmax(ys[t])  # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    \n",
    "  \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxi, dWhi, dbi = np.zeros_like(Wxi), np.zeros_like(Whi), np.zeros_like(bi)\n",
    "    dWxf, dWhf, dbf = np.zeros_like(Wxf), np.zeros_like(Whf), np.zeros_like(bf)\n",
    "    dWxo, dWho, dbo = np.zeros_like(Wxo), np.zeros_like(Who), np.zeros_like(bo)\n",
    "    dWxc, dWhc, dbc = np.zeros_like(Wxc), np.zeros_like(Whc), np.zeros_like(bc)\n",
    "    dWhy, dby = np.zeros_like(Why), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dcnext = np.zeros_like(cs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1  # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        do = dh * np.tanh(cs[t])\n",
    "        do = do * o_s[t] * (1 - o_s[t]) # backprop through output gate # if do = do * sigmoid'(o)\n",
    "\n",
    "        dc = dh * o_s[t] * (1 - np.tanh(cs[t]) ** 2) + dcnext # backprop through cell state\n",
    "        di = dc * c_tildes[t]\n",
    "        di = di * i_s[t] * (1 - i_s[t]) # backprop through input gate sigmoid' = sigmoid * (1 - sigmoid)\n",
    "        df = dc * cs[t-1]\n",
    "        df = df * f_s[t] * (1 - f_s[t])  # backprop through forget gate\n",
    "        dc_tilde = dc * i_s[t]\n",
    "        dc_tilde = dc_tilde * (1 - c_tildes[t] ** 2) # backprop through input node\n",
    "\n",
    "        # accumulate gradients for weights and biases\n",
    "        dWxi += np.dot(di, xs[t].T)\n",
    "        dWhi += np.dot(di, hs[t-1].T)\n",
    "        dbi += di\n",
    "\n",
    "        dWxf += np.dot(df, xs[t].T)\n",
    "        dWhf += np.dot(df, hs[t-1].T)\n",
    "        dbf += df\n",
    "\n",
    "        dWxo += np.dot(do, xs[t].T)\n",
    "        dWho += np.dot(do, hs[t-1].T)\n",
    "        dbo += do\n",
    "\n",
    "        dWxc += np.dot(dc_tilde, xs[t].T)\n",
    "        dWhc += np.dot(dc_tilde, hs[t-1].T)\n",
    "        dbc += dc_tilde\n",
    "\n",
    "        # pass gradients to the next time step\n",
    "        dhnext = np.dot(Whi.T, di) + np.dot(Whf.T, df) + np.dot(Who.T, do) + np.dot(Whc.T, dc_tilde)\n",
    "        dcnext = dc * f_s[t]\n",
    "\n",
    "    return loss, dWxi, dWhi, dbi, dWxf, dWhf, dbf, dWxo, dWho, dbo, dWxc, dWhc, dbc, dWhy, dby, hs[len(inputs)-1], cs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleLTSM(h, c, seed_ix, n):\n",
    "    \"\"\" \n",
    "    Sample a sequence of integers from the model \n",
    "    h is the hidden state, c is the cell state, seed_ix is the seed letter for the first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        # LSTM forward pass for one time step\n",
    "        i = sigmoid(np.dot(Wxi, x) + np.dot(Whi, h) + bi)\n",
    "        f = sigmoid(np.dot(Wxf, x) + np.dot(Whf, h) + bf)\n",
    "        o = sigmoid(np.dot(Wxo, x) + np.dot(Who, h) + bo)\n",
    "        c_tilde = np.tanh(np.dot(Wxc, x) + np.dot(Whc, h) + bc)\n",
    "        c = f * c + i * c_tilde\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = softmax(y)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " st all one sour mind.\n",
      "\n",
      "LUCEI aTWHODEONa:\n",
      "What is good?\n",
      "\n",
      "CEPRUYT:\n",
      "No; huse this tay you serve,\n",
      "And comsun be sinct aly you's man; you, what news,\n",
      "Sut an chereest me that he prose playsed\n",
      "That they ib s \n",
      "----\n",
      "iter 198900, loss: 38.450987\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Memory variables for Adagrad\n",
    "mWxi, mWhi, mbi = np.zeros_like(Wxi), np.zeros_like(Whi), np.zeros_like(bi)\n",
    "mWxf, mWhf, mbf = np.zeros_like(Wxf), np.zeros_like(Whf), np.zeros_like(bf)\n",
    "mWxo, mWho, mbo = np.zeros_like(Wxo), np.zeros_like(Who), np.zeros_like(bo)\n",
    "mWxc, mWhc, mbc = np.zeros_like(Wxc), np.zeros_like(Whc), np.zeros_like(bc)\n",
    "mWhy, mby = np.zeros_like(Why), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "# Loss at iteration 0\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length \n",
    "\n",
    "n, p = 0, 0\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        cprev = np.zeros((hidden_size, 1))  # reset cell state\n",
    "        p = 0 # go from start of data\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length + 1]]\n",
    "    \n",
    "    # Forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxi, dWhi, dbi, dWxf, dWhf, dbf, dWxo, dWho, dbo, dWxc, dWhc, dbc, dWhy, dby, hprev, cprev = lossFunLTSM(inputs, targets, hprev, cprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: \n",
    "        #sample from the model now and then \n",
    "        clear_output(wait=True)  # Clears the output cell before printing new info\n",
    "        sample_ix = sampleLTSM(hprev, cprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, )) \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # Perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxi, Whi, bi, Wxf, Whf, bf, Wxo, Who, bo, Wxc, Whc, bc, Why, by],\n",
    "                                  [dWxi, dWhi, dbi, dWxf, dWhf, dbf, dWxo, dWho, dbo, dWxc, dWhc, dbc, dWhy, dby],\n",
    "                                  [mWxi, mWhi, mbi, mWxf, mWhf, mbf, mWxo, mWho, mbo, mWxc, mWhc, mbc, mWhy, mby]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # Adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter \n",
    "\n",
    "    if n == 44700 * 5: # 5 times pass the full text \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THEORY OFFTOPIC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a **step-by-step** explanation of the math behind **backpropagation** in the LSTM snippet you provided. We’ll walk through each gate (input, forget, output, candidate cell state), how the hidden/cell states update, and finally how we accumulate gradients by going backward in time.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Forward Pass Recap\n",
    "\n",
    "In your forward pass, at each time step $ t $:\n",
    "\n",
    "1. **One-hot input**:  \n",
    "   $$\n",
    "   x_s[t] \\in \\mathbb{R}^{\\text{vocab\\_size}\\times 1}.\n",
    "   $$\n",
    "   You also have a hidden state $ h_s[t-1] $ and cell state $ c_s[t-1] $.\n",
    "\n",
    "2. **Gates** (all dimension $\\text{hidden} \\times 1$):\n",
    "\n",
    "   - **Input Gate**:  \n",
    "     $$\n",
    "     i_t = \\sigma(W_{xi} \\, x_s[t] + W_{hi}\\, h_s[t-1] + b_i).\n",
    "     $$\n",
    "   - **Forget Gate**:  \n",
    "     $$\n",
    "     f_t = \\sigma(W_{xf} \\, x_s[t] + W_{hf} \\, h_s[t-1] + b_f).\n",
    "     $$\n",
    "   - **Output Gate**:  \n",
    "     $$\n",
    "     o_t = \\sigma(W_{xo} \\, x_s[t] + W_{ho} \\, h_s[t-1] + b_o).\n",
    "     $$\n",
    "   - **Candidate Cell** ($ \\tilde{c}_t $):  \n",
    "     $$\n",
    "     \\tilde{c}_t = \\tanh(W_{xc} \\, x_s[t] + W_{hc}\\, h_s[t-1] + b_c).\n",
    "     $$\n",
    "\n",
    "3. **Cell State Update**:  \n",
    "   $$\n",
    "   c_s[t] = f_t \\odot c_s[t-1] \\;+\\; i_t \\odot \\tilde{c}_t.\n",
    "   $$\n",
    "   ($\\odot$ = elementwise multiply)\n",
    "\n",
    "4. **Hidden State Update**:  \n",
    "   $$\n",
    "   h_s[t] = o_t \\odot \\tanh(c_s[t]).\n",
    "   $$\n",
    "\n",
    "5. **Output (logits)**:  \n",
    "   $$\n",
    "   y_s[t] = W_{hy}\\, h_s[t] + b_y,\n",
    "   $$\n",
    "   then probabilities via softmax $ p_s[t] = \\mathrm{softmax}(y_s[t]) $.\n",
    "\n",
    "6. **Loss** (cross-entropy):  \n",
    "   $$\n",
    "   \\text{loss} = -\\sum_{t} \\log(p_s[t]_{(\\text{target}[t])}).\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Backprop Through Output Layer\n",
    "\n",
    "### 2.1. Gradient wrt. $ y_s[t] $\n",
    "\n",
    "**Softmax with cross-entropy** gives the typical “logits” gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{loss}}{\\partial y_s[t]} \n",
    "= p_s[t] - \\mathbf{1}\\{\\text{target}[t]\\}.\n",
    "$$\n",
    "In your code:\n",
    "\n",
    "```python\n",
    "dy = np.copy(ps[t])\n",
    "dy[targets[t]] -= 1\n",
    "```\n",
    "This array `dy` is $\\partial L / \\partial y_s[t]$.\n",
    "\n",
    "### 2.2. Hidden State to Output Weight\n",
    "\n",
    "Then:\n",
    "$$\n",
    "dWhy \\;+=\\; dy \\; h_s[t]^T,\n",
    "\\quad\n",
    "dby \\;+=\\; dy.\n",
    "$$\n",
    "Because:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{hy}} \n",
    "= \\frac{\\partial L}{\\partial y_s[t]} \\cdot \\frac{\\partial y_s[t]}{\\partial W_{hy}}\n",
    "= dy \\, (h_s[t])^T.\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_y} \n",
    "= dy.\n",
    "$$\n",
    "\n",
    "### 2.3. Backprop into hidden state\n",
    "\n",
    "$$\n",
    "dh_t = W_{hy}^T \\, dy \\;+\\; dh_{\\text{next}}.\n",
    "$$\n",
    "In your code:\n",
    "```python\n",
    "dh = np.dot(Why.T, dy) + dhnext\n",
    "```\n",
    "Because you have to add any gradient coming from future timesteps $(dh_{\\text{next}})$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Backprop Through LSTM Cell (Single Timestep)\n",
    "\n",
    "Let’s see how to get $\\partial L/\\partial h_s[t-1]$ and $\\partial L/\\partial c_s[t-1]$ from $\\partial L/\\partial h_s[t]$ and $\\partial L/\\partial c_s[t]$. We do this for each gate: input, forget, output, plus candidate cell.\n",
    "\n",
    "### 3.1. Decompose $\\partial L/\\partial h_s[t]$\n",
    "\n",
    "We had:\n",
    "\n",
    "```python\n",
    "dh = np.dot(Why.T, dy) + dhnext\n",
    "```\n",
    "Now we break that into partial derivatives for the gates. The hidden state is:\n",
    "$$\n",
    "h_s[t] = o_t \\odot \\tanh(c_s[t]).\n",
    "$$\n",
    "So:\n",
    "\n",
    "$$\n",
    "dh_s[t] = dh.\n",
    "$$\n",
    "We define:\n",
    "$$\n",
    "do_t = dh \\odot \\tanh(c_s[t])\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "d(\\tanh(c_s[t])) = dh \\odot o_t\n",
    "$$\n",
    "\n",
    "But more explicitly, since:\n",
    "$$\n",
    "h_s[t] = o_t \\odot \\tanh(c_s[t]),\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial h_s[t]}{\\partial o_t} = \\tanh(c_s[t]),\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial h_s[t]}{\\partial c_s[t]} = o_t \\cdot (1-\\tanh^2(c_s[t])).\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "1. $\\displaystyle do = dh \\odot \\tanh(c_s[t])$.\n",
    "2. $\\displaystyle dc_s[t] = dh \\odot o_t \\odot (1 - \\tanh^2(c_s[t]))$.\n",
    "\n",
    "In your code:\n",
    "\n",
    "```python\n",
    "do = dh * np.tanh(cs[t])\n",
    "do = do * o * (1 - o)  # if do = do * sigmoid'(o)\n",
    "```\n",
    "But wait, that second line is actually the derivative of the output gate **sigmoid**. So we have a chain rule:\n",
    "\n",
    "$$\n",
    "do \\;=\\; do \\;\\times\\; \\sigma'(o_t).\n",
    "$$\n",
    "where $\\sigma'(z) = z(1 - z)$ for the sigmoid gate. That’s the line:\n",
    "\n",
    "```python\n",
    "do = do * o * (1 - o)\n",
    "```\n",
    "**(Which is** $\\partial o_t / \\partial (pre\\_act\\_o)$**).**\n",
    "\n",
    "### 3.2. Cell State Derivatives\n",
    "\n",
    "**Now** we have partial derivative w.r.t. $ c_s[t] $:\n",
    "```python\n",
    "dc = dh * o * (1 - np.tanh(cs[t])**2) + dcnext\n",
    "```\n",
    "This is effectively:\n",
    "$$\n",
    "dc_s[t] = \\frac{\\partial L}{\\partial c_s[t]} = dh \\odot o_t \\odot (1-\\tanh^2(c_s[t])) + dc_{\\text{next}}.\n",
    "$$\n",
    "- The first part is from the direct chain of $\\partial h_s[t]/ \\partial c_s[t]$.\n",
    "- We also add in $\\partial L/\\partial c_s[t]$ from the next time step, $dc_{\\text{next}}$. Because the cell state c[t] also flows to c[t+1].\n",
    "\n",
    "### 3.3. Input, Forget gates, and Candidate\n",
    "\n",
    "Recall:\n",
    "\n",
    "$$\n",
    "c_s[t] = f_t \\odot c_s[t-1] + i_t \\odot \\tilde{c}_t.\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "- $\\partial c_s[t]/\\partial f_t = c_s[t-1]$.\n",
    "- $\\partial c_s[t]/\\partial i_t = \\tilde{c}_t$.\n",
    "- $\\partial c_s[t]/\\partial \\tilde{c}_t = i_t$.\n",
    "\n",
    "**Inside code**:\n",
    "\n",
    "1. We have a variable `dc`, which is the total gradient wrt. `c_s[t]`.\n",
    "2. So:\n",
    "   ```python\n",
    "   di = dc * c_tilde\n",
    "   df = dc * cs[t-1]\n",
    "   dc_tilde = dc * i\n",
    "   ```\n",
    "   Then we apply the gate activation derivatives:\n",
    "\n",
    "   - For the input gate: `i = sigmoid(...)`, so $\\partial i/\\partial z = i(1-i)$.\n",
    "     ```python\n",
    "     di = di * i * (1 - i)\n",
    "     ```\n",
    "   - For the forget gate: `f = sigmoid(...)`.\n",
    "     ```python\n",
    "     df = df * f * (1 - f)\n",
    "     ```\n",
    "   - For candidate cell: `c_tilde = tanh(...)`.\n",
    "     ```python\n",
    "     dc_tilde = dc_tilde * (1 - c_tilde**2)\n",
    "     ```\n",
    "\n",
    "### 3.4. Summaries of Gate Derivatives\n",
    "\n",
    "So we get partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial (pre\\_act\\_i)} = di,\\quad\n",
    "\\frac{\\partial L}{\\partial (pre\\_act\\_f)} = df,\\quad\n",
    "\\frac{\\partial L}{\\partial (pre\\_act\\_o)} = do,\\quad\n",
    "\\frac{\\partial L}{\\partial (pre\\_act\\_c\\_tilde)} = dc\\_tilde.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Accumulate Weight/Bias Gradients\n",
    "\n",
    "Each gate has parameters $(W_{x\\_gate}, W_{h\\_gate}, b_{\\_gate})$. For example, the input gate uses `Wxi`, `Whi`, `bi`. We do:\n",
    "\n",
    "```python\n",
    "dWxi += np.dot(di, xs[t].T)\n",
    "dWhi += np.dot(di, hs[t-1].T)\n",
    "dbi   += di\n",
    "```\n",
    "This follows from:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{xi}} \n",
    "= \\sum_t \\frac{\\partial L}{\\partial (pre\\_act\\_i)} \\,\\cdot\\, x_s[t]^T\n",
    "$$\n",
    "since `(pre_act_i) = W_{xi} * x_s[t] + W_{hi} * h_s[t-1] + b_i`.\n",
    "\n",
    "Similarly for `f, o, c_tilde` gates.\n",
    "\n",
    "### 4.1. Pass Gradients Back for Next Timestep\n",
    "\n",
    "After we compute gate derivatives at time $ t$, we have:\n",
    "\n",
    "$$\n",
    "dh_{\\text{next}} = \\frac{\\partial L}{\\partial h_s[t-1]}\n",
    "$$\n",
    "$$\n",
    "dc_{\\text{next}} = \\frac{\\partial L}{\\partial c_s[t-1]}\n",
    "$$\n",
    "so the code sets:\n",
    "\n",
    "```python\n",
    "dhnext = np.dot(Whi.T, di) + np.dot(Whf.T, df) + ...\n",
    "dcnext = dc * f\n",
    "```\n",
    "Why? Because\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_s[t-1]}\n",
    "= \\sum_{g \\in \\{i,f,o,c\\_tilde\\}} W_{hg}^T \\; \\frac{\\partial L}{\\partial (pre\\_act\\_g)}\n",
    "$$\n",
    "plus any contributions from other paths if relevant. And for the cell state:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial c_s[t-1]}\n",
    "= \\frac{\\partial L}{\\partial c_s[t]} \\odot f_t.\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "```python\n",
    "dhnext = np.dot(Whi.T, di) + np.dot(Whf.T, df) + ...\n",
    "dcnext = dc * f\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Clipping the Gradients\n",
    "\n",
    "At the end,\n",
    "\n",
    "```python\n",
    "for dparam in [...]:\n",
    "    np.clip(dparam, -5, 5, out=dparam)\n",
    "```\n",
    "**just** ensures each gradient stays in $[-5, 5]$ range to mitigate exploding gradients in RNNs. This doesn’t affect the chain rule itself; it’s a practical measure.\n",
    "\n",
    "---\n",
    "\n",
    "### **In Summary**\n",
    "\n",
    "1. **Forward**: LSTM gates = $\\sigma$ or $\\tanh$. Hidden/cell update equations. Output log probabilities.  \n",
    "2. **Backward**:\n",
    "   - Start from the output layer error $\\Delta y_s[t]$. Accumulate into `Why`, `by`.  \n",
    "   - Propagate into hidden state $\\Delta h_s[t]$.  \n",
    "   - Break $\\Delta h_s[t]$ into partial derivatives for output gate, cell state.  \n",
    "   - Then $\\Delta c_s[t]$ is combined with `dcnext` from the next step.  \n",
    "   - Compute input gate, forget gate, candidate cell partials.  \n",
    "   - Each gate’s gradient flows to the corresponding weight matrix.  \n",
    "   - Compute $\\Delta h_s[t-1]$ and $\\Delta c_s[t-1]$ for the next iteration.  \n",
    "3. **Clip** the final gradients to avoid exploding.  \n",
    "\n",
    "This chain rule ensures each parameter of each gate receives the correct partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
