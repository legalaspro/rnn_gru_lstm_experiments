{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try write here the GRU version lol \n",
    "# hyperparameters\n",
    "hidden_size = 512 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters initialization\n",
    "def init_weight(*shape):\n",
    "    fan_in = shape[1] if len(shape) > 1 else shape[0]\n",
    "    return np.random.randn(*shape) * np.sqrt(2.0 / fan_in)  # He initialization\n",
    "\n",
    "def triple():\n",
    "    \"\"\"\n",
    "    Returns (Wx, Wh, b) each gate param\n",
    "    hidden_size x vocab_size, hidden_size x hidden_size, hidden_size x 1\n",
    "    \"\"\"\n",
    "    return (init_weight(hidden_size, vocab_size),  # input -> hidden\n",
    "            init_weight(hidden_size, hidden_size), # hidden -> hidden\n",
    "            np.zeros((hidden_size, 1)))            # bias\n",
    "\n",
    "Wxz, Whz, bz = triple()  # Update gate\n",
    "Wxr, Whr, br = triple()  # Reset gate\n",
    "Wxh, Whh, bh = triple()  # Candidate hidden state\n",
    "\n",
    "Why = init_weight(vocab_size, hidden_size)  # hidden -> output\n",
    "by = np.zeros((vocab_size, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gru.png\" alt=\"GRU \"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "def lossFunGRU(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets: list of integers (token indices).\n",
    "    hprev: hidden_size x 1 array, initial hidden state.\n",
    "    Returns: (loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby, hlast)\n",
    "    where hlast is the hidden state after processing the last character.\n",
    "    \"\"\"\n",
    "    xs, hs, zs, rs, h_tildes, ys, ps = {}, {}, {}, {}, {}, {}, {}  # Dictionaries contain variables for each timestep.\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        # 1-of-k input\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "\n",
    "        # GRU forward\n",
    "        # Update gate z\n",
    "        z_t = sigmoid(np.dot(Wxz, xs[t]) + np.dot(Whz, hs[t-1]) + bz)\n",
    "        # Reset gate r\n",
    "        r_t = sigmoid(np.dot(Wxr, xs[t]) + np.dot(Whr, hs[t-1]) + br)\n",
    "        # Candidate hidden\n",
    "        h_tilde = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, r_t * hs[t-1]) + bh) #use np.multiply instead of *?\n",
    "\n",
    "        # Final hidden\n",
    "        h_t = z_t * hs[t-1] + (1 - z_t) * h_tilde\n",
    "\n",
    "        zs[t] = z_t\n",
    "        rs[t] = r_t\n",
    "        h_tildes[t] = h_tilde\n",
    "        hs[t] = h_t\n",
    "\n",
    "        # Output\n",
    "        y_t = np.dot(Why, h_t) + by # unnormalized log probabilities for next chars (vocab, 1)\n",
    "        ys[t] = y_t\n",
    "        p_t = softmax(y_t)  # probabilities for next chars\n",
    "        ps[t] = p_t\n",
    "        loss += -np.log(p_t[targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    \n",
    "  \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxz, dWhz, dbz = np.zeros_like(Wxz), np.zeros_like(Whz), np.zeros_like(bz)\n",
    "    dWxr, dWhr, dbr = np.zeros_like(Wxr), np.zeros_like(Whr), np.zeros_like(br)\n",
    "    dWxh, dWhh, dbh = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(bh)\n",
    "    dWhy, dby = np.zeros_like(Why), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0]) # gradient wrt next hidden\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Output layer gradient\n",
    "        # ‚àÇloss/‚àÇy = p - 1 (softmax gradient)\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1  # softmax gradient\n",
    "\n",
    "        # ‚àÇloss/‚àÇWy and ‚àÇloss/‚àÇby\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "\n",
    "        # Hidden gradient\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # from output + next time step\n",
    "\n",
    "        # Retrieve gates\n",
    "        z_t = zs[t]\n",
    "        r_t = rs[t]\n",
    "        h_tilde = h_tildes[t]\n",
    "        h_prev = hs[t-1]\n",
    "\n",
    "        # Gradient wrt final h_t = z_t * h_{t-1} + (1-z_t)*h_tilde\n",
    "        # => partial wrt h_tilde: (1 - z_t)  \n",
    "        dh_tilde = dh * (1 - z_t)\n",
    "        dh_tilde = dh_tilde * (1 - h_tilde**2)  # backprop through tanh\n",
    "\n",
    "        # ‚àÇloss/‚àÇWh, ‚àÇloss/‚àÇUh and ‚àÇloss/‚àÇbh\n",
    "        # candidate hidden\n",
    "        # h_tilde = tanh(Wxh*x + Whh*(r_t * h_prev) + bh)\n",
    "        dWxh += np.dot(dh_tilde, xs[t].T)\n",
    "        dWhh += np.dot(dh_tilde, (r_t * h_prev).T)\n",
    "        dbh += dh_tilde\n",
    "        \n",
    "        # Gate derivatives wrt final h_t = z_t * h_{t-1} + (1-z_t)*h_tilde\n",
    "        # => partial wrt z_t: (h_prev - h_tilde)\n",
    "        dz = dh * (h_prev - h_tilde)\n",
    "        dz = dz * z_t * (1 - z_t)  # backprop through sigmoid z\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWz, ‚àÇloss/‚àÇUz and ‚àÇloss/‚àÇbz\n",
    "        dWxz += np.dot(dz, xs[t].T)\n",
    "        dWhz += np.dot(dz, h_prev.T)\n",
    "        dbz += dz\n",
    "\n",
    "        # reset gate partial\n",
    "        # h_tilde depends on (r_t * h_prev)\n",
    "        # so partial wrt r_t is (Whh * h_prev) dot the dh_tilde part\n",
    "        drhp = np.dot(Whh.T, dh_tilde)\n",
    "        # but we want partial wrt r_t => dr_part * dh_tilde => then pass through sig.\n",
    "        # Actually we want the part from the \"pre-act\" of h_tilde:\n",
    "        #   h_tilde = tanh(Wxh x + Whh(r_t*h_prev) + bh)\n",
    "        #   derivative wrt r_t => (Whh * h_prev) * dh_tilde\n",
    "        dr = drhp * h_prev  # another approach\n",
    "        # => next we do reset gate derivative\n",
    "        dr = dr * r_t * (1 - r_t)\n",
    "       \n",
    "        # r gate\n",
    "        dWxr += np.dot(dr, xs[t].T)\n",
    "        dWhr += np.dot(dr, h_prev.T)\n",
    "        dbr += dr\n",
    "\n",
    "        \n",
    "        # pass gradient back to h_{t-1}\n",
    "        # from final eq: partial wrt h_prev is dh * z_t\n",
    "        # from z gate partial: np.dot(Whz.T, dz)\n",
    "        # from r gate partial: np.dot(Whr.T, dr)\n",
    "        #   also we have (Whh.T * dh_tilde) * r_t as partial wrt h_prev\n",
    "        dh_fz_inner = np.dot(Whz.T, dz)\n",
    "        dh_fz = dh * z_t\n",
    "        dh_fhh = drhp * r_t\n",
    "        dh_fr = np.dot(Whr.T, dr)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇhùë°‚Çã‚ÇÅ\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr # for next iteration\n",
    "        \n",
    "\n",
    "    # return everything\n",
    "    return (loss, dWxz, dWhz, dbz,\n",
    "            dWxr, dWhr, dbr,\n",
    "            dWxh, dWhh, dbh,\n",
    "            dWhy, dby,\n",
    "            hs[len(inputs)-1])  # last hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleGRU(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    Sample a sequence of integers from the model \n",
    "    h is the hidden state, c is the cell state, seed_ix is the seed letter for the first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    h = np.copy(h)\n",
    "    for _ in range(n):\n",
    "        # GRU forward pass for one time step\n",
    "        z = sigmoid(np.dot(Wxz, x) + np.dot(Whz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wxr, x) + np.dot(Whr, h) + br)\n",
    "        h_tilde = np.tanh(np.dot(Wxh, x) + np.dot(Whh, (r * h)) + bh)\n",
    "        h = z * h + (1 - z) * h_tilde\n",
    "\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = softmax(y)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  your lus, it on him: bring,\n",
      "If his hilds that chunks, shall veny lisal one\n",
      "San incexion undo steeling. This atwands in\n",
      "Buckinghous, look and thenteats fall: mush\n",
      "Hast full of must that well;\n",
      "The proy \n",
      "----\n",
      "iter 223400, loss: 37.050309\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Memory variables for Adagrad\n",
    "mWxz, mWhz, mbz = np.zeros_like(Wxz), np.zeros_like(Whz), np.zeros_like(bz)\n",
    "mWxr, mWhr, mbr = np.zeros_like(Wxr), np.zeros_like(Whr), np.zeros_like(br)\n",
    "mWxh, mWhh, mbh = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(bh)\n",
    "mWhy, mby = np.zeros_like(Why), np.zeros_like(by)\n",
    "\n",
    "# Loss at iteration 0\n",
    "smooth_loss = -np.log(1.0/vocab_size) * seq_length\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1))\n",
    "        p = 0\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length + 1]]\n",
    "    \n",
    "    # Forward seq_length characters through the net and fetch gradient\n",
    "    (loss, dWxz, dWhz, dbz,\n",
    "     dWxr, dWhr, dbr,\n",
    "     dWxh, dWhh, dbh,\n",
    "     dWhy, dby,\n",
    "     hprev) = lossFunGRU(inputs, targets, hprev)\n",
    "    \n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0:\n",
    "        #sample from the model now and then \n",
    "        clear_output(wait=True)  # Clears the output cell before printing new info\n",
    "        sample_ix = sampleGRU(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))  \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # Perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxz, Whz, bz,\n",
    "                                   Wxr, Whr, br,\n",
    "                                   Wxh, Whh, bh,\n",
    "                                   Why, by],\n",
    "                                  [dWxz, dWhz, dbz,\n",
    "                                   dWxr, dWhr, dbr,\n",
    "                                   dWxh, dWhh, dbh,\n",
    "                                   dWhy, dby],\n",
    "                                  [mWxz, mWhz, mbz,\n",
    "                                   mWxr, mWhr, mbr,\n",
    "                                   mWxh, mWhh, mbh,\n",
    "                                   mWhy, mby]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter \n",
    "\n",
    "    if n == 44700 * 5: # 5 times pass the full text \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  age in myself: be the heartest Masialin--\n",
      "Vill om cort worls me basts, for the fraitlo-spaked?\n",
      "But nuturg ill gentleman: be hame? Fired.\n",
      "\n",
      "GocZ:\n",
      "You are you comes percuing and buin, destraition.\n",
      "\n",
      "MIshNCat:\n",
      "'Twas first they!\n",
      "\n",
      "KANGANINA:\n",
      "In the gotelous' drum. Steed sase me apprace,\n",
      "And oo but that he dod bessows not point,\n",
      "Then be arunge them fill menolany.\n",
      "\n",
      "JOLIES:\n",
      "When, till I lear this.\n",
      "\n",
      "KING EDWARD IV:\n",
      "And on they.\n",
      "\n",
      "CORIOLONUS:\n",
      "He grownded temps? I knock.\n",
      "They leams the seave with care and ol \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test_sample = sampleGRU(hprev, 15, 500)\n",
    "txt = ''.join(ix_to_char[ix] for ix in test_sample)\n",
    "print('----\\n %s \\n----' % (txt, )) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THEORY OFFTOPIC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a **step-by-step** explanation of the **GRU** (Gated Recurrent Unit) forward pass and corresponding backpropagation (chain rule) ‚Äî in a style similar to the LSTM explanation. We'll walk through each gate (update, reset), how the hidden state is computed, and how gradients flow backward.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. GRU Forward Pass Recap\n",
    "\n",
    "For a single time step $ t $, with input vector $ x_t \\in \\mathbb{R}^{\\text{in\\_dim}} $ and previous hidden state $ h_{t-1} \\in \\mathbb{R}^{\\text{hidden}} $, a GRU does:\n",
    "\n",
    "1. **Update Gate** $ z_t $:\n",
    "   $$\n",
    "   z_t = \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z).\n",
    "   $$\n",
    "   This gate decides **how much** of the previous hidden state is carried over (like the LSTM \"forget\" and \"input\" gates combined).\n",
    "\n",
    "2. **Reset Gate** $ r_t $:\n",
    "   $$\n",
    "   r_t = \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r).\n",
    "   $$\n",
    "   This gate decides how much of the **past hidden** information to ‚Äúreset‚Äù or ignore in computing the candidate state.\n",
    "\n",
    "3. **Candidate Hidden State** $ \\tilde{h}_t $:\n",
    "   $$\n",
    "   \\tilde{h}_t = \\tanh\\bigl(W_{x\\tilde{h}} x_t + b_{\\tilde{h}x} + \n",
    "   r_t \\odot (W_{h\\tilde{h}}\\, h_{t-1} + b_{\\tilde{h}h})\\bigr).\n",
    "   $$\n",
    "   - The reset gate $r_t$ is applied **elementwise** to $h_{t-1}$ before computing $\\tilde{h}_t$.\n",
    "   - $\\tilde{h}_t$ is the new candidate state that might partially replace the old hidden state.\n",
    "\n",
    "4. **Final Hidden State** $ h_t $:\n",
    "   $$\n",
    "   h_t = (1 - z_t) \\odot \\tilde{h}_t + z_t \\odot h_{t-1}.\n",
    "   $$\n",
    "   - If $z_t$ is close to 1, we keep most of the old $h_{t-1}$. If $z_t$ is close to 0, we replace it more with $\\tilde{h}_t$.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Detailed Forward Equations\n",
    "\n",
    "Collecting them:\n",
    "\n",
    "1. **Update Gate**:  \n",
    "   $$\n",
    "   z_t = \\sigma(U_z x_t + W_z h_{t-1} + b_z).\n",
    "   $$\n",
    "2. **Reset Gate**:  \n",
    "   $$\n",
    "   r_t = \\sigma(U_r x_t + W_r h_{t-1} + b_r).\n",
    "   $$\n",
    "3. **Candidate** ($\\tilde{h}_t$):  \n",
    "   $$\n",
    "   \\tilde{h}_t = \\tanh\\bigl(U_{\\tilde{h}} x_t + b_{\\tilde{h}x} + \n",
    "             r_t \\odot (W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h})\\bigr).\n",
    "   $$\n",
    "4. **Hidden State**:  \n",
    "   $$\n",
    "   h_t = z_t \\odot h_{t-1} \\;+\\; (1 - z_t)\\odot \\tilde{h}_t.\n",
    "   $$\n",
    "\n",
    "*(Some references arrange parameters differently, but the essence is the same. We‚Äôll just keep consistent notation in the derivation.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Backprop Through a Single Timestep\n",
    "\n",
    "We want to compute $\\frac{\\partial L}{\\partial x_t}$, $\\frac{\\partial L}{\\partial h_{t-1}}$, and $\\frac{\\partial L}{\\partial \\Theta}$ (all the weight matrices/biases), given $\\frac{\\partial L}{\\partial h_t}$. This is the chain rule. Let‚Äôs define:\n",
    "\n",
    "$$\n",
    "dh_t \\equiv \\frac{\\partial L}{\\partial h_t}\n",
    "$$\n",
    "\n",
    "### 3.1. Hidden State Splitting\n",
    "\n",
    "$$\n",
    "h_t = z_t \\odot h_{t-1} + (1 - z_t)\\odot \\tilde{h}_t.\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial z_t} \n",
    "= h_{t-1} \\;-\\; \\tilde{h}_t, \n",
    "\\quad\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} \n",
    "= z_t, \n",
    "\\quad\n",
    "\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\n",
    "= (1 - z_t).\n",
    "$$\n",
    "\n",
    "So we get partial derivatives:\n",
    "\n",
    "1. $\\displaystyle dz_t = dh_t \\odot (h_{t-1} - \\tilde{h}_t)$.\n",
    "2. $\\displaystyle d h_{t-1}^{(from\\_h)} = dh_t \\odot z_t$.\n",
    "3. $\\displaystyle d\\tilde{h}_t = dh_t \\odot (1 - z_t)$.\n",
    "\n",
    "*(We‚Äôll also accumulate a separate gradient to $h_{t-1}$ from inside $\\tilde{h}_t$‚Äôs formula, so the final $\\frac{\\partial L}{\\partial h_{t-1}}$ is a sum of multiple terms.)*\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. Candidate Hidden $\\tilde{h}_t$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh\\Bigl(U_{\\tilde{h}} x_t + b_{\\tilde{h}x} + \n",
    "            r_t \\odot (W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h})\\Bigr).\n",
    "$$\n",
    "\n",
    "From the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial (pre\\_act)} \n",
    "= (1 - \\tilde{h}_t^2),\n",
    "$$\n",
    "where $(pre\\_act)$ is the linear combination inside the $\\tanh$. So:\n",
    "\n",
    "1. Let $\\displaystyle d\\tilde{h}_t = \\Delta \\equiv \\frac{\\partial L}{\\partial \\tilde{h}_t}$.\n",
    "2. $\\displaystyle d(pre\\_act) = \\Delta \\odot (1 - \\tilde{h}_t^2).$\n",
    "\n",
    "Now, $(pre\\_act) = U_{\\tilde{h}} x_t + b_{\\tilde{h}x} + r_t \\odot (W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h}).$\n",
    "\n",
    "We see that inside we have **two** big chunks:\n",
    "\n",
    "- $U_{\\tilde{h}} x_t + b_{\\tilde{h}x}$.\n",
    "- $r_t \\odot (W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h})$.\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (pre\\_act)}{\\partial x_t} \n",
    "= U_{\\tilde{h}}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\frac{\\partial (pre\\_act)}{\\partial h_{t-1}} \n",
    "= r_t \\odot W_{\\tilde{h}}\n",
    "\\quad\n",
    "\\frac{\\partial (pre\\_act)}{\\partial r_t} \n",
    "= (W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h}).\n",
    "$$\n",
    "\n",
    "So we define:\n",
    "\n",
    "```python\n",
    "dpre_act = dtilde_h * (1 - tilde_h**2)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "dU_h += np.dot(dpre_act, x_t^T)\n",
    "db_hx += dpre_act  # sum over batch if needed\n",
    "dW_h  += ...\n",
    "db_hh += ...\n",
    "```\n",
    "(We‚Äôll see the details below, also we note the partial wrt. r_t is used to get dr_t.)\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. The Reset Gate $ r_t $\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "r_t = \\sigma(U_r x_t + W_r h_{t-1} + b_r).\n",
    "$$\n",
    "\n",
    "But it also appears in $\\tilde{h}_t$ as a multiplier for $(W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h})$. So to find $\\partial L/\\partial r_t$, we see:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (pre\\_act)}{\\partial r_t} \n",
    "= W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h}.\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "dr_t\n",
    "= \\bigl[\\underbrace{(W_{\\tilde{h}} h_{t-1} + b_{\\tilde{h}h})}_{(\\ast)}\\bigr] \\odot dpre\\_act,\n",
    "$$\n",
    "since it‚Äôs an elementwise multiply in the forward pass. Then we do a ‚Äúsigmoid derivative‚Äù:\n",
    "\n",
    "$$\n",
    "dr_t \\;\\times\\; r_t \\; (1 - r_t).\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "```python\n",
    "dr = (W_hh h_{t-1} + b_hh) * dpre_act\n",
    "dr = dr * r * (1 - r)\n",
    "```\n",
    "*(Symbol names are approximate to your actual code variables.)*\n",
    "\n",
    "### 3.4. Summarize Grad Flows to $h_{t-1}$ from Candidate & Reset\n",
    "\n",
    "- Part from the direct (pre_act) partial:\n",
    "\n",
    "  $$\n",
    "  d h_{t-1}^{(\\tilde{h})} \n",
    "  = (r_t) \\odot W_{\\tilde{h}}^T \\cdot dpre\\_act\n",
    "  $$\n",
    "- Part from the reset gate derivative itself:\n",
    "\n",
    "  $$\n",
    "  dr_t \n",
    "  = ...\n",
    "  \\quad\\Rightarrow\\quad\n",
    "  d h_{t-1}^{(reset)} \n",
    "  = W_r^T \\, dr_t\n",
    "  $$\n",
    "\n",
    "So total derivative for $ h_{t-1} $ from the candidate path:\n",
    "\n",
    "$$\n",
    "d h_{t-1}^{(\\text{candidate part})}\n",
    "= r_t \\odot (W_{\\tilde{h}}^T \\cdot dpre\\_act) + W_r^T \\, dr_t.\n",
    "$$\n",
    "\n",
    "(Then we also have a term from the final hidden state eq, i.e. $ d h_{t-1}^{(\\text{carry over})} = dh_t \\odot z_t $.)\n",
    "\n",
    "### 3.5. Update Gate $ z_t $\n",
    "\n",
    "$$\n",
    "z_t = \\sigma(U_z x_t + W_z h_{t-1} + b_z).\n",
    "$$\n",
    "We have from the final hidden eq:\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial z_t}\n",
    "= h_{t-1} - \\tilde{h}_t.\n",
    "$$\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "dz_t = dh_t \\odot (h_{t-1} - \\tilde{h}_t).\n",
    "$$\n",
    "Then the gate derivative:\n",
    "\n",
    "$$\n",
    "dz_t \\leftarrow dz_t \\;\\times\\; z_t(1 - z_t)\n",
    "$$\n",
    "*(since $z_t$ is a sigmoid).*\n",
    "\n",
    "We accumulate that to find partials wrt. $ x_t $ and $ h_{t-1} $ via $U_z$ and $W_z$. So:\n",
    "\n",
    "```python\n",
    "dU_z += np.dot(dz_t, x_t^T)\n",
    "dW_z += np.dot(dz_t, h_{t-1}^T)\n",
    "db_z += dz_t\n",
    "```\n",
    "And for the hidden state portion from `z_t`:\n",
    "\n",
    "$$\n",
    "d h_{t-1}^{(update)} = W_z^T \\, dz_t\n",
    "$$\n",
    "\n",
    "### 3.6. Combining All Paths to $h_{t-1}$\n",
    "\n",
    "So the final gradient wrt. $ h_{t-1} $ is the sum from:\n",
    "\n",
    "1. The partial w.r.t. the final hidden eq: $dh_t \\odot z_t$.  \n",
    "2. The partial from the candidate $\\tilde{h}$ chain: $(r_t \\odot W_{\\tilde{h}}^T \\cdot dpre\\_act) + W_r^T dr_t$.  \n",
    "3. The partial from the update gate: $W_z^T dz_t$.\n",
    "\n",
    "Hence in code, you might do something like:\n",
    "\n",
    "```python\n",
    "dh_t_minus_1 = (dh_t * z_t)\n",
    "             + (r_t * np.dot(W_hc.T, dpre_act))   # candidate path\n",
    "             + np.dot(W_r.T, dr)\n",
    "             + np.dot(W_z.T, dz)\n",
    "```\n",
    "\n",
    "*(Where variable names match your usage. Symbolic references vary in code but the logic is the same.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Gradient Accumulation: Weights/Bias\n",
    "\n",
    "For each gate $(z, r, \\tilde{h})$, we do:\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial U_{(\\cdot)}} \\;+=\\; \\text{(gradient wrt. gate pre-act)} \\times x_t^T.$\n",
    "2. $\\frac{\\partial L}{\\partial W_{(\\cdot)}} \\;+=\\; \\text{(gradient wrt. gate pre-act)} \\times h_{t-1}^T.$\n",
    "3. $\\frac{\\partial L}{\\partial b_{(\\cdot)}} \\;+=\\; \\text{(gradient wrt. gate pre-act)}.$\n",
    "\n",
    "Where ‚Äúgate pre-act‚Äù is the derivative after applying the elementwise activation derivative. (For example, `dz_t`, `dr_t`, `dpre_act` for candidate.)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Multiple Timesteps (BPTT)\n",
    "\n",
    "We just described a single time step‚Äôs backward pass. In an RNN (or GRU) you do **Backprop Through Time**:\n",
    "\n",
    "- Start from $\\frac{\\partial L}{\\partial h_{T}}$ = 0 for final step (or from the loss if you have a final MLP).\n",
    "- Move backward from $t=T$ down to $t=1$. \n",
    "- At each step, you compute partial derivatives for that step‚Äôs gate operations, add them to the global gradient buffers (dU_z, dW_z, etc.), then propagate $\\frac{\\partial L}{\\partial h_{t-1}}$ to the next step.  \n",
    "\n",
    "This is basically what your code snippet does in the reversed loop:\n",
    "\n",
    "```python\n",
    "for t in reversed(range(len(inputs))):\n",
    "    # compute d y_s[t], accumulate into W_hy, b_y\n",
    "    # compute dh, do, dc, di, df, etc. or for GRU: dz, dr, dtilde_h\n",
    "    # pass dhnext, dcnext to the next iteration\n",
    "```\n",
    "\n",
    "**But** in a GRU, we only track hidden states $(h_t)$, no separate cell state like LSTM. So we pass `dhnext`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summation / Clipping\n",
    "\n",
    "Finally, after we accumulate all gradients $\\frac{\\partial L}{\\partial \\theta}$, we often do gradient **clipping**. For example:\n",
    "\n",
    "```python\n",
    "for dparam in [dW_z, dU_z, db_z, dW_r, dU_r, db_r, dW_h, dU_h, db_h, ...]:\n",
    "    np.clip(dparam, -5, 5, out=dparam)\n",
    "```\n",
    "\n",
    "This ensures we keep the gradient magnitudes from exploding.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of GRU Derivation**\n",
    "\n",
    "1. **Forward**:\n",
    "   - Compute gates $ z_t, r_t $ using $\\sigma$.  \n",
    "   - Compute candidate $\\tilde{h}_t$ with $\\tanh$.  \n",
    "   - Combine to get $h_t = z_t \\odot h_{t-1} + (1-z_t)\\odot \\tilde{h}_t$.\n",
    "\n",
    "2. **Backward** (per-step):\n",
    "   - $\\frac{\\partial L}{\\partial h_t}$ is known from the next stage (or final output).  \n",
    "   - Decompose w.r.t. each gate: \n",
    "     - $z_t$ derivative from $\\partial h_t / \\partial z_t$.  \n",
    "     - $\\tilde{h}_t$ derivative from $\\partial h_t / \\partial \\tilde{h}_t$.  \n",
    "     - inside $\\tilde{h}_t$ chain, also compute $\\partial r_t$.  \n",
    "   - Sum partials for $\\partial h_{t-1}$.  \n",
    "   - Accumulate weight/bias grads for each gate.  \n",
    "\n",
    "3. **Iterate backwards** across time steps (BPTT).\n",
    "\n",
    "4. **Optional**: clip gradients to mitigate exploding.  \n",
    "\n",
    "This matches your LSTM snippet‚Äôs style, but with GRU‚Äôs simpler gating structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_zero_to_hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
